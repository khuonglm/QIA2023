{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rXYWHZ9qkRWq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmUA6eO3pkUz"},"outputs":[],"source":["!pip install transformers==4.28.0\n","!pip install datasets evaluate\n","# !pip install konlpy"]},{"cell_type":"markdown","metadata":{"id":"Z_oI22Ndp-mO"},"source":["# Importing Package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKR_LQbYp71h"},"outputs":[],"source":["import collections\n","import numpy as np\n","import string\n","import pandas as pd\n","\n","import logging\n","import json\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","\n","import evaluate\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BertTokenizerFast,\n","    AlbertModel,\n","    DataCollatorWithPadding,\n","    PreTrainedTokenizerFast,\n","    TrainingArguments,\n","    Trainer,\n","    DefaultDataCollator,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGmdbAIAb_fL"},"outputs":[],"source":["\n","#config\n","\n","COMBINE_SIZE = 2\n","RANDOM_SEED = 80\n","TRAIN_PATH = '/content/drive/MyDrive/QIA2023_phase2/data/train_data.xlsx'\n","TEST_PATH = '/content/drive/MyDrive/QIA2023_phase2/data/test_data.xlsx'"]},{"cell_type":"markdown","metadata":{"id":"MGgWBYBxqcAO"},"source":["# Loading Data + Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R82qF9sbiX0_"},"outputs":[],"source":["df = pd.read_excel(TRAIN_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjqVB50qDwRi"},"outputs":[],"source":["df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aKbxbNJitQm"},"outputs":[],"source":["print(len(df['User_ID'].unique()))\n","print(df['User_ID'].unique())\n","\n","import random\n","user_id_list = df['User_ID'].unique()\n","random.shuffle(user_id_list)"]},{"cell_type":"markdown","metadata":{"id":"zh34HV9sSNnr"},"source":["# Loading pretrained model checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kKgF739BSQ-d"},"outputs":[],"source":["# I = 0, E = 1\n","# S = 0, N = 1\n","# J = 0, P = 1\n","# T = 0, F = 1\n","label_class = [['I', 'E'], ['S', 'N'], ['T', 'F'], ['J', 'P']]\n","\n","labels = [\"ESTJ\", \"ENTJ\", \"ESFJ\", \"ENFJ\", \"ISTJ\", \"ISFJ\", \"INTJ\", \"INFJ\", \"ESTP\", \"ESFP\", \"ENTP\", \"ENFP\", \"ISTP\", \"ISFP\", \"INTP\", \"INFP\"]\n","\n","id2label = dict()\n","label2id = dict()\n","\n","for i, label in enumerate(labels):\n","  label2id[label] = i\n","  id2label[i] = label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4r2kX0FqQ5i"},"outputs":[],"source":["question_df = pd.read_excel('/content/drive/MyDrive/QIA2023_phase2/data/Question.xlsx')['Question']\n","\n","def remove_trash(short_ans):\n","  if '/' in short_ans:\n","      temp = short_ans.split('/')\n","      return temp[0]\n","  return short_ans\n","\n","def read_df(df):\n","  user_infos = dict()\n","\n","  for index, row in df.iterrows():\n","    id = row['User_ID']\n","\n","    if (id not in user_infos):\n","      user_infos[id] = {\n","          'Age': row['Age'],\n","          'Gender': row['Gender'],\n","          'questions': [],\n","          'responses': [],\n","          'MBTI': row['MBTI']\n","      }\n","\n","    user_infos[id]['questions'].append(question_df[row['Q_number'] - 1])\n","    user_infos[id]['responses'].append(remove_trash(row['Short_Answer']) + ' , ' + row['Long_Answer'])\n","    \n","  return user_infos\n","\n","def read_test_df(df):\n","  user_infos = dict()\n","\n","  for index, row in df.iterrows():\n","    id = row['User_ID']\n","\n","    if (id not in user_infos):\n","      user_infos[id] = {\n","          'Age': row['Age'],\n","          'Gender': row['Gender'],\n","          'questions': [],\n","          'responses': []\n","      }\n","\n","    user_infos[id]['questions'].append(question_df[row['Q_number'] - 1])\n","    user_infos[id]['responses'].append(remove_trash(row['Short_Answer']) + ' , ' + row['Long_Answer'])\n","    \n","  return user_infos\n","\n","def get_dataset(data_df):\n","  user_info = read_df(data_df)\n","  \n","\n","  dataset = {\n","      \"text\": [],\n","      \"label\": [],\n","  }\n","\n","  for id, data in user_info.items():\n","    questions = data['questions']\n","    responses = data['responses']\n","    label_str = data['MBTI']\n","    info_str = str(data['Age']) + ' [SEP] ' + data['Gender']\n","    size = len(questions)\n","\n","    for i in range(0, size, COMBINE_SIZE):\n","      if (i + COMBINE_SIZE) > size:\n","        break\n","      \n","      text = '질문 : ' + questions[i] + ' [SEP] ' + '답변 : ' + responses[i]\n","      for j in range(1, COMBINE_SIZE, 1):\n","        text += ' [SEP] ' + '질문 : ' + questions[i + j] + ' [SEP] ' + '답변 : ' + responses[i + j]\n","      text += ' [SEP] ' + info_str     \n","\n","      dataset['text'].append(text);\n","      dataset['label'].append(label2id[label_str])\n","\n","  return datasets.Dataset.from_dict(dataset)\n","  # return datasets.DatasetDict({\"train\":datasets.Dataset.from_dict(train_dataset),\"test\":datasets.Dataset.from_dict(val_dataset)})\n","\n","def get_test_dataset(test_df):\n","  user_info = read_test_df(test_df)\n","  \n","  user_ids = []\n","  result = []\n","  \n","  for id, data in user_info.items():\n","    questions = data['questions']\n","    responses = data['responses']\n","    info_str = str(data['Age']) + ' [SEP] ' + data['Gender']\n","    size = len(questions)\n","\n","    for i in range(0, size, COMBINE_SIZE):\n","      if (i + COMBINE_SIZE) > size:\n","        break\n","      text = '질문 : ' + questions[i] + ' [SEP] ' + '답변 : ' + responses[i]\n","      for j in range(1, COMBINE_SIZE, 1):\n","        text += ' [SEP] ' + '질문 : ' + questions[i + j] + ' [SEP] ' + '답변 : ' + responses[i + j]\n","      text += ' [SEP] ' + info_str\n","      result.append(text)\n","      \n","    user_ids.append(id)\n","  return result, user_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8e3jEmWsbKI"},"outputs":[],"source":["# dataset = {\n","#     \"text\": [],\n","#     \"label\": []\n","# }\n","\n","# for index, row in df.iterrows():\n","#     dataset[\"text\"].append(row[\"Long_Answer\"])\n","#     dataset[\"label\"].append(label2id[row[\"MBTI\"]])\n","# df = pd.read_csv(TRAIN_PATH, encoding=\"cp949\", index_col=0)\n","df = pd.read_excel(TRAIN_PATH)\n","dataset = get_dataset(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y24A5QWfshhg"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","dataset = dataset.train_test_split(test_size = 0.1, seed=RANDOM_SEED)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IzcvcDzskTO"},"outputs":[],"source":["example = dataset['train'][0]\n","example"]},{"cell_type":"markdown","metadata":{"id":"nzUiEc9HSWWF"},"source":["# TensorFlow model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SEZXfhYGblT"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n","from transformers import AutoModelForMaskedLM\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from transformers import TFBertModel\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","# Configuration\n","EPOCHS = 3\n","BATCH_SIZE = 8\n","MAX_LEN = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCRiERmCH6EP"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"-mW5uMeWSftj"},"source":["# Hugging Face model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTnbqj7JRidK"},"outputs":[],"source":["# model_checkpoint = \"klue/roberta-large\"\n","# model_checkpoint = \"klue/bert-base\"\n","model_checkpoint = \"beomi/kcbert-large\"\n","# model_checkpoint = \"/content/drive/MyDrive/QIA2023_phase2/temp-checkpoints/kcbert-large/checkpoint-8500-lr1e5-031\"\n","# model_checkpoint = \"beomi/kcbert-large\"\n","\n","config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kr01OkLS_GWQ"},"outputs":[],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmZP7zxOL6S-"},"outputs":[],"source":["# dataset = get_dataset()\n","# dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQ5PUe3ZGqUR"},"outputs":[],"source":["# example = dataset['train'][0]\n","# example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mg0Rdji5LdTH"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcwHhmelPVL2"},"outputs":[],"source":["tokenized_ds = dataset.map(preprocess_function, batched = True)"]},{"cell_type":"markdown","metadata":{"id":"EqhF2-6aSfYt"},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L50iQUPXO4WI"},"outputs":[],"source":["accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{"id":"QnOr0VAUSi4e"},"source":["# Class Training Arguments + Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUnsu8vySee1"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKT0178TPBhY"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir = \"/content/drive/MyDrive/QIA2023_phase2/temp-checkpoints/kcbert-large\",\n","    overwrite_output_dir = 'True',\n","    learning_rate = 1e-5,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = 20,\n","    weight_decay = 0.01,\n","    evaluation_strategy = \"steps\",\n","    save_strategy = \"steps\",\n","    eval_steps = 200,\n","    save_steps = 200,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omhENsE3PDAw"},"outputs":[],"source":["trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = tokenized_ds[\"train\"],\n","    eval_dataset = tokenized_ds[\"test\"],\n","    tokenizer = tokenizer,\n","    data_collator = data_collator,\n","    compute_metrics = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yV3q96LJva8"},"outputs":[],"source":["import torch, gc\n","import os\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzjtP02uhIc3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lZtY8wwfSm39"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru6C2WeoPfi-"},"outputs":[],"source":["trainer.train()\n","# trainer.save_model(\"checkpoints\")"]},{"cell_type":"markdown","metadata":{"id":"M2Hh11rQSpNg"},"source":["# Deploy Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdUTphn4RevG"},"outputs":[],"source":["from tqdm import tqdm\n","import torch, gc\n","from transformers import DataCollatorWithPadding\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7he3uXBmYE7g"},"outputs":[],"source":["test_df = pd.read_excel(TEST_PATH)\n","# test_df = preprocess_data(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BmAIUWxGYJ9w"},"outputs":[],"source":["model_checkpoint = '/content/drive/MyDrive/QIA2023_phase2/temp-checkpoints/kcbert-large/checkpoint-2000'\n","\n","config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=300)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",").to(device)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i-0pEiJlYMBz"},"outputs":[],"source":["# texts = []\n","\n","# for index, row in test_df.iterrows():\n","#     texts.append(row['Long_Answer'])\n","# len(texts)\n","\n","texts, user_ids = get_test_dataset(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5LVlmv4Fmbv9"},"outputs":[],"source":["texts[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RANwtArN7HK7"},"outputs":[],"source":["raw_logits = {\n","    'I/E': [],\n","    'S/N': [],\n","    'T/F': [],\n","    'J/P': []\n","}\n","\n","final_logits = {\n","    'User_ID': user_ids,\n","    'I/E': [],\n","    'S/N': [],\n","    'T/F': [],\n","    'J/P': []\n","}\n","\n","for i in tqdm(range(0, len(texts), 16)):\n","    current_batch_size = min(16, len(texts) - i)\n","\n","    inputs = texts[i: i + current_batch_size]\n","    inputs = tokenizer(inputs, truncation=True)\n","    inputs = data_collator(inputs).to(device)\n","\n","    logits = model(**inputs).logits\n","    logits = torch.nn.Softmax(dim = 1)(logits)\n","    logits = torch.permute(logits, (1, 0)).cpu().data\n","    one_logits = torch.zeros(4, current_batch_size)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    for label in labels:\n","        if ('E' in label):  one_logits[0] += logits[label2id[label]]\n","        if ('N' in label):  one_logits[1] += logits[label2id[label]]\n","        if ('F' in label):  one_logits[2] += logits[label2id[label]]\n","        if ('P' in label):  one_logits[3] += logits[label2id[label]]\n","    \n","    raw_logits['I/E'] += one_logits[0].tolist()\n","    raw_logits['S/N'] += one_logits[1].tolist()\n","    raw_logits['T/F'] += one_logits[2].tolist()\n","    raw_logits['J/P'] += one_logits[3].tolist()\n","\n","combine_steps = int(60 / COMBINE_SIZE)\n","\n","def calc(row):\n","  cnt = 0\n","  for x in row:\n","    if x >= 0.5:\n","      cnt += 1\n","  if cnt >= int(len(row) / 2):\n","    return max(row)\n","  return min(row)\n","\n","for i in tqdm(range(0, len(texts), combine_steps)):\n","    final_logits['I/E'] += [calc(raw_logits['I/E'][i:i+combine_steps])]\n","    final_logits['S/N'] += [calc(raw_logits['S/N'][i:i+combine_steps])]\n","    final_logits['T/F'] += [calc(raw_logits['T/F'][i:i+combine_steps])]\n","    final_logits['J/P'] += [calc(raw_logits['J/P'][i:i+combine_steps])]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3hlw2nOJQ1wr"},"outputs":[],"source":["result = pd.DataFrame(final_logits)\n","result.index += 1\n","result.to_csv('/content/drive/MyDrive/to_submit/Results/Phase2/kcbert_combine_2_ver2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9QtXyxRtP-c"},"outputs":[],"source":["# result = pd.DataFrame(raw_logits)\n","# result.index += 1\n","# result.to_csv('/content/drive/MyDrive/QIA2023_phase2/result/raw_kcbert_combine_2_2000_phase2only_lr1e5_v2.csv', index_label=\"User_ID\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"la7eFmwBYPeI"},"outputs":[],"source":["# test = pd.DataFrame({\n","#     'I/E': [MBTI[0] for MBTI in MBTIs], \n","#     'S/N': [MBTI[1] for MBTI in MBTIs], \n","#     'T/F': [MBTI[2] for MBTI in MBTIs], \n","#     'J/P': [MBTI[3] for MBTI in MBTIs], \n","# })\n","# test.index += 1\n","# test.to_csv(f'/content/drive/MyDrive/QIA2023_phase1/result/single-flow-1/result-{checkpoint_idx}.csv', index_label=\"idx\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhhErpBk-f4i"},"outputs":[],"source":["# %cp -av /content/Single-Flow-model/checkpoint-9000 /content/drive/MyDrive/QIA2023_phase1/checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnP5nZqTBPvR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}