{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOtbOCG6D6/NReRUgw77qm+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rXYWHZ9qkRWq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers==4.28.0\n","!pip install datasets evaluate\n","!pip install konlpy"],"metadata":{"id":"gmUA6eO3pkUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing Package"],"metadata":{"id":"Z_oI22Ndp-mO"}},{"cell_type":"code","source":["import collections\n","import numpy as np\n","import string\n","import pandas as pd\n","\n","import logging\n","import json\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","\n","import evaluate\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BertTokenizerFast,\n","    AlbertModel,\n","    DataCollatorWithPadding,\n","    PreTrainedTokenizerFast,\n","    TrainingArguments,\n","    Trainer,\n","    DefaultDataCollator,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version"],"metadata":{"id":"qKR_LQbYp71h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Data + Preprocessing"],"metadata":{"id":"MGgWBYBxqcAO"}},{"cell_type":"code","source":["# get stopwords list\n","stopwords_file_path = '/content/drive/MyDrive/QIA2023_phase1/data/stopwords.txt'\n","lines = open(stopwords_file_path, \"r\")\n","\n","filter_list = []\n","for word in lines:\n","  filter_list.append(word.replace(\"\\n\", \"\").replace(\"\\ufeff\", \"\"))\n","filter_list.append(\".\")\n","filter_list.append(\",\")\n","\n","from konlpy.tag import Okt\n","from konlpy.utils import pprint\n","\n","okt = Okt()\n","\n","# remove stopwords and punctuation\n","def preprocess_str(text):\n","  text = okt.morphs(text, norm=True) # not lemma\n","  text = [word for word in text if word not in filter_list]\n","  result = text[0]\n","  for word in text[1:]:\n","    result = result + \" \" + word\n","  return result\n","\n","question_df = pd.read_excel('/content/drive/MyDrive/QIA2023_phase1/data/Question.xlsx')['Question']\n","# question_df = question_df.apply(preprocess_str)\n","\n","\n","# Remove <> and apply preprocess_str\n","def preprocess_ans(word):\n","  lst = word.split(\">\")\n","  return lst[1] + \" [SEP]\" + lst[0][1:]\n","\n","def preprocess_data(data_df):\n","  data_df[\"Answer\"] = data_df[\"Answer\"].apply(preprocess_ans)\n","  data_df[\"Q_number\"] = data_df[\"Q_number\"].apply(lambda idx: question_df[idx - 1])\n","  data_df['Answer'] = data_df[\"Q_number\"] + \" [SEP] \" + data_df['Answer'] + \" [SEP] \" + data_df['Age'].astype(str) + \" [SEP] \" + data_df['Gender'].astype(str)\n","  return data_df"],"metadata":{"id":"D4r2kX0FqQ5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load datasets\n","df = pd.read_csv('/content/drive/MyDrive/QIA2023_phase1/data/train_data_s1_v1.csv', encoding='cp949', index_col=0)\n","df = preprocess_data(df)"],"metadata":{"id":"R82qF9sbiX0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.tail()"],"metadata":{"id":"VjqVB50qDwRi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading pretrained model checkpoint"],"metadata":{"id":"zh34HV9sSNnr"}},{"cell_type":"code","source":["labels = [\"ESTJ\", \"ENTJ\", \"ESFJ\", \"ENFJ\", \"ISTJ\", \"ISFJ\", \"INTJ\", \"INFJ\", \"ESTP\", \"ESFP\", \"ENTP\", \"ENFP\", \"ISTP\", \"ISFP\", \"INTP\", \"INFP\"]\n","\n","id2label = dict()\n","label2id = dict()\n","\n","for i, label in enumerate(labels):\n","  label2id[label] = i\n","  id2label[i] = label"],"metadata":{"id":"kKgF739BSQ-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TensorFlow model"],"metadata":{"id":"nzUiEc9HSWWF"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from transformers import TFBertModel\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","# Configuration\n","EPOCHS = 3\n","BATCH_SIZE = 8\n","MAX_LEN = 512"],"metadata":{"id":"8SEZXfhYGblT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hugging Face model"],"metadata":{"id":"-mW5uMeWSftj"}},{"cell_type":"code","source":["model_checkpoint = \"klue/roberta-large\"\n","\n","config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",")"],"metadata":{"id":"TTnbqj7JRidK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = {\n","    \"text\": [],\n","    \"label\": []\n","}\n","\n","for index, row in df.iterrows():\n","    dataset[\"text\"].append(row[\"Answer\"])\n","    dataset[\"label\"].append(label2id[row[\"MBTI\"]])\n","\n","dataset = datasets.Dataset.from_dict(dataset)"],"metadata":{"id":"FPaVp4FTL7Oz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","dataset = dataset.train_test_split(test_size = 0.05)"],"metadata":{"id":"TmZP7zxOL6S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example = dataset['train'][0]\n","example"],"metadata":{"id":"cQ5PUe3ZGqUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation = True)"],"metadata":{"id":"Mg0Rdji5LdTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_ds = dataset.map(preprocess_function, batched = True)"],"metadata":{"id":"UcwHhmelPVL2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Metrics"],"metadata":{"id":"EqhF2-6aSfYt"}},{"cell_type":"code","source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"L50iQUPXO4WI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Class Training Arguments + Trainer"],"metadata":{"id":"QnOr0VAUSi4e"}},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"hUnsu8vySee1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir = \"checkpoints\",\n","    overwrite_output_dir = 'True',\n","    learning_rate = 1e-5,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = 10,\n","    weight_decay = 0.01,\n","    evaluation_strategy = \"steps\",\n","    save_strategy = \"steps\",\n","    eval_steps = 1000\n",")"],"metadata":{"id":"CKT0178TPBhY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = tokenized_ds[\"train\"],\n","    eval_dataset = tokenized_ds[\"test\"],\n","    tokenizer = tokenizer,\n","    data_collator = data_collator,\n","    compute_metrics = compute_metrics,\n",")"],"metadata":{"id":"omhENsE3PDAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch, gc\n","import os\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"6yV3q96LJva8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"lZtY8wwfSm39"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"ru6C2WeoPfi-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deploy Model"],"metadata":{"id":"M2Hh11rQSpNg"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"cdUTphn4RevG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = pd.read_csv('/content/drive/MyDrive/QIA2023_phase1/data/hackathon_test_for_user.csv', encoding='cp949')\n","test_df = preprocess_data(test_df)"],"metadata":{"id":"7he3uXBmYE7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"/content/checkpoints/checkpoint-7500\""],"metadata":{"id":"8BUbmwW3YIHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",").to(device)\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"9XdeYZvvNt7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = []\n","\n","for index, row in test_df.iterrows():\n","    texts.append(row['Answer'])"],"metadata":{"id":"6TrePQ9oZYma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_logits = {\n","    'I/E': [],\n","    'S/N': [],\n","    'T/F': [],\n","    'J/P': []\n","}\n","\n","for i in tqdm(range(0, len(texts), 16)):\n","    current_batch_size = min(16, len(texts) - i)\n","\n","    inputs = texts[i: i + current_batch_size]\n","    inputs = tokenizer(inputs)\n","    inputs = data_collator(inputs).to(device)\n","\n","    logits = model(**inputs).logits\n","    logits = torch.nn.Softmax(dim = 1)(logits)\n","    logits = torch.permute(logits, (1, 0)).cpu().data\n","    one_logits = torch.zeros(4, current_batch_size)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    for label in labels:\n","        if ('E' in label):  one_logits[0] += logits[label2id[label]]\n","        if ('N' in label):  one_logits[1] += logits[label2id[label]]\n","        if ('F' in label):  one_logits[2] += logits[label2id[label]]\n","        if ('P' in label):  one_logits[3] += logits[label2id[label]]\n","    \n","    final_logits['I/E'] += one_logits[0].tolist()\n","    final_logits['S/N'] += one_logits[1].tolist()\n","    final_logits['T/F'] += one_logits[2].tolist()\n","    final_logits['J/P'] += one_logits[3].tolist()"],"metadata":{"id":"WuN8KLojQign"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pd.DataFrame(final_logits)\n","result.index += 1\n","result.to_csv('/content/drive/MyDrive/to_submit/Results/Phase1/klue-roberta-large.csv', index_label=\"idx\")"],"metadata":{"id":"0v2MyU4Ogo1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test = pd.DataFrame({\n","#     'I/E': [0 if MBTI[0] == 'I' else 1 for MBTI in MBTIs], \n","#     'S/N': [0 if MBTI[1] == 'S' else 1 for MBTI in MBTIs], \n","#     'T/F': [0 if MBTI[2] == 'T' else 1 for MBTI in MBTIs], \n","#     'J/P': [0 if MBTI[3] == 'J' else 1 for MBTI in MBTIs], \n","# })\n","# test.index += 1\n","# test.to_csv('result.csv', index_label=\"idx\")"],"metadata":{"id":"la7eFmwBYPeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cp -av /content/Single-Flow-model/checkpoint-7500 /content/drive/MyDrive/QIA2023_phase1/checkpoint"],"metadata":{"id":"ORZWO-DXJxOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NACNX3mvJ2Dz"},"execution_count":null,"outputs":[]}]}