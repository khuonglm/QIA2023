{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rXYWHZ9qkRWq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers==4.28.0\n","!pip install datasets evaluate\n","!pip install konlpy"],"metadata":{"id":"gmUA6eO3pkUz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing Package"],"metadata":{"id":"Z_oI22Ndp-mO"}},{"cell_type":"code","source":["import collections\n","import numpy as np\n","import string\n","import pandas as pd\n","\n","import logging\n","import json\n","import os\n","import sys\n","from dataclasses import dataclass, field\n","from typing import Optional\n","\n","import datasets\n","from datasets import load_dataset\n","\n","import evaluate\n","import transformers\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    BertTokenizerFast,\n","    AlbertModel,\n","    DataCollatorWithPadding,\n","    PreTrainedTokenizerFast,\n","    TrainingArguments,\n","    Trainer,\n","    DefaultDataCollator,\n","    default_data_collator,\n","    set_seed,\n",")\n","from transformers.trainer_utils import get_last_checkpoint\n","from transformers.utils import check_min_version, send_example_telemetry\n","from transformers.utils.versions import require_version"],"metadata":{"id":"qKR_LQbYp71h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading Data + Preprocessing"],"metadata":{"id":"MGgWBYBxqcAO"}},{"cell_type":"code","source":["RANDOM_SEED = 80\n","\n","# get stopwords list\n","stopwords_file_path = '/content/drive/MyDrive/QIA2023_phase1/data/stopwords.txt'\n","lines = open(stopwords_file_path, \"r\")\n","\n","filter_list = []\n","for word in lines:\n","  filter_list.append(word.replace(\"\\n\", \"\").replace(\"\\ufeff\", \"\"))\n","filter_list.append(\".\")\n","filter_list.append(\",\")\n","\n","from konlpy.tag import Okt\n","from konlpy.utils import pprint\n","\n","okt = Okt()\n","\n","# remove stopwords and punctuation\n","def preprocess_str(text):\n","  text = okt.morphs(text, norm=True) # not lemma\n","  text = [word for word in text if word not in filter_list]\n","  result = text[0]\n","  for word in text[1:]:\n","    result = result + \" \" + word\n","  return result\n","\n","question_df = pd.read_excel('/content/drive/MyDrive/QIA2023_phase1/data/Question.xlsx')['Question']\n","# question_df = question_df.apply(preprocess_str)\n","\n","\n","# Remove <> and apply preprocess_str\n","def preprocess_ans(word):\n","  lst = word.split(\">\")\n","  return lst[1] + \" [SEP] \" + lst[0][1:]\n","\n","def preprocess_data(data_df):\n","  data_df[\"Answer\"] = data_df[\"Answer\"].apply(preprocess_ans)\n","  data_df[\"Q_number\"] = data_df[\"Q_number\"].apply(lambda idx: question_df[idx - 1])\n","  data_df['Answer'] = data_df[\"Q_number\"] + \" [SEP] \" + data_df['Answer'] + \" [SEP] \" + data_df['Age'].astype(str) + \" [SEP] \" + data_df['Gender'].astype(str)\n","  return data_df"],"metadata":{"id":"D4r2kX0FqQ5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load datasets\n","df = pd.read_csv('/content/drive/MyDrive/QIA2023_phase1/data/train_data_s1_v1.csv', encoding='cp949', index_col=0)\n","df = preprocess_data(df)"],"metadata":{"id":"R82qF9sbiX0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.tail()"],"metadata":{"id":"VjqVB50qDwRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split train and test dataframe\n","train_df_list = []\n","val_df_list = []\n","for idx in df['User_ID'].unique():\n","    train_df_list.append(df[df['User_ID']==idx][0:45])\n","    val_df_list.append(df[df['User_ID']==idx][45:])"],"metadata":{"id":"qrgKP0ue6MeW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.concat(train_df_list, ignore_index=True)\n","val_df = pd.concat(val_df_list, ignore_index=True)"],"metadata":{"id":"J-ESFSso6N9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = train_df.sample(frac=1, random_state=RANDOM_SEED)\n","train_df.tail()"],"metadata":{"id":"kepJberm6PXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_df = val_df.sample(frac=1, random_state=RANDOM_SEED)\n","val_df.tail()"],"metadata":{"id":"tXnX32lr6WlT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading pretrained model checkpoint"],"metadata":{"id":"zh34HV9sSNnr"}},{"cell_type":"code","source":["# I = 0, E = 1\n","# S = 0, N = 1\n","# J = 0, P = 1\n","# T = 0, F = 1\n","label_class = [['I', 'E'], ['S', 'N'], ['T', 'F'], ['J', 'P']]\n","\n","labels = [\"ESTJ\", \"ENTJ\", \"ESFJ\", \"ENFJ\", \"ISTJ\", \"ISFJ\", \"INTJ\", \"INFJ\", \"ESTP\", \"ESFP\", \"ENTP\", \"ENFP\", \"ISTP\", \"ISFP\", \"INTP\", \"INFP\"]\n","\n","id2label = dict()\n","label2id = dict()\n","\n","for i, label in enumerate(labels):\n","  label2id[label] = i\n","  id2label[i] = label"],"metadata":{"id":"kKgF739BSQ-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dataset():\n","  train_dataset = {\n","      \"text\": [],\n","      \"label\": []\n","  }\n","\n","  for index, row in train_df.iterrows():\n","      train_dataset[\"text\"].append(row[\"Answer\"])\n","      train_dataset[\"label\"].append(label2id[row[\"MBTI\"]])\n","\n","  val_dataset = {\n","      \"text\": [],\n","      \"label\": []\n","  }\n","\n","  for index, row in val_df.iterrows():\n","      val_dataset[\"text\"].append(row[\"Answer\"])\n","      val_dataset[\"label\"].append(label2id[row[\"MBTI\"]])\n","\n","  return datasets.DatasetDict({\"train\":datasets.Dataset.from_dict(train_dataset),\"test\":datasets.Dataset.from_dict(val_dataset)})"],"metadata":{"id":"R0nxh7lF6bxj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TensorFlow model"],"metadata":{"id":"nzUiEc9HSWWF"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n","from transformers import AutoModelForMaskedLM\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from transformers import TFBertModel\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","# Configuration\n","EPOCHS = 3\n","BATCH_SIZE = 8\n","MAX_LEN = 512"],"metadata":{"id":"8SEZXfhYGblT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"yCRiERmCH6EP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hugging Face model"],"metadata":{"id":"-mW5uMeWSftj"}},{"cell_type":"code","source":["# model_checkpoint = \"klue/roberta-large\"\n","# model_checkpoint = \"klue/bert-base\"\n","model_checkpoint = \"beomi/kcbert-large\"\n","\n","config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",")"],"metadata":{"id":"TTnbqj7JRidK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"id":"Kr01OkLS_GWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = get_dataset()\n","dataset"],"metadata":{"id":"TmZP7zxOL6S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example = dataset['train'][0]\n","example"],"metadata":{"id":"cQ5PUe3ZGqUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation = True)"],"metadata":{"id":"Mg0Rdji5LdTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_ds = dataset.map(preprocess_function, batched = True)"],"metadata":{"id":"UcwHhmelPVL2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Metrics"],"metadata":{"id":"EqhF2-6aSfYt"}},{"cell_type":"code","source":["accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"L50iQUPXO4WI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Class Training Arguments + Trainer"],"metadata":{"id":"QnOr0VAUSi4e"}},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"hUnsu8vySee1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir = \"checkpoints\",\n","    overwrite_output_dir = 'True',\n","    learning_rate = 1e-5,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = 50,\n","    weight_decay = 0.0,\n","    evaluation_strategy = \"steps\",\n","    save_strategy = \"steps\",\n","    eval_steps = 500,\n",")"],"metadata":{"id":"CKT0178TPBhY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = tokenized_ds[\"train\"],\n","    eval_dataset = tokenized_ds[\"test\"],\n","    tokenizer = tokenizer,\n","    data_collator = data_collator,\n","    compute_metrics = compute_metrics,\n",")"],"metadata":{"id":"omhENsE3PDAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch, gc\n","import os\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"6yV3q96LJva8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"lZtY8wwfSm39"}},{"cell_type":"code","source":["trainer.train()\n","# trainer.save_model(\"checkpoints\")"],"metadata":{"id":"ru6C2WeoPfi-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deploy Model"],"metadata":{"id":"M2Hh11rQSpNg"}},{"cell_type":"code","source":["from tqdm import tqdm\n","import torch, gc\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"cdUTphn4RevG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = pd.read_csv('/content/drive/MyDrive/QIA2023_phase1/data/hackathon_test_for_user.csv', encoding='cp949')\n","test_df = preprocess_data(test_df)"],"metadata":{"id":"7he3uXBmYE7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checkpoint_idx = 18000\n","# model_checkpoint = f\"/content/drive/MyDrive/QIA2023_phase1/checkpoint/checkpoint-{checkpoint_idx}\""],"metadata":{"id":"8BUbmwW3YIHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = '/content/checkpoints/checkpoint-500'\n","\n","config    = AutoConfig.from_pretrained(model_checkpoint)\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","model     = AutoModelForSequenceClassification.from_pretrained(\n","    model_checkpoint, num_labels=16, id2label=id2label, label2id=label2id\n",").to(device)"],"metadata":{"id":"BmAIUWxGYJ9w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = []\n","\n","for index, row in test_df.iterrows():\n","    texts.append(row['Answer'])"],"metadata":{"id":"i-0pEiJlYMBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_logits = {\n","    'I/E': [],\n","    'S/N': [],\n","    'T/F': [],\n","    'J/P': []\n","}\n","\n","for i in tqdm(range(0, len(texts), 16)):\n","    current_batch_size = min(16, len(texts) - i)\n","\n","    inputs = texts[i: i + current_batch_size]\n","    inputs = tokenizer(inputs)\n","    inputs = data_collator(inputs).to(device)\n","\n","    logits = model(**inputs).logits\n","    logits = torch.nn.Softmax(dim = 1)(logits)\n","    logits = torch.permute(logits, (1, 0)).cpu().data\n","    one_logits = torch.zeros(4, current_batch_size)\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    for label in labels:\n","        if ('E' in label):  one_logits[0] += logits[label2id[label]]\n","        if ('N' in label):  one_logits[1] += logits[label2id[label]]\n","        if ('F' in label):  one_logits[2] += logits[label2id[label]]\n","        if ('P' in label):  one_logits[3] += logits[label2id[label]]\n","    \n","    final_logits['I/E'] += one_logits[0].tolist()\n","    final_logits['S/N'] += one_logits[1].tolist()\n","    final_logits['T/F'] += one_logits[2].tolist()\n","    final_logits['J/P'] += one_logits[3].tolist()"],"metadata":{"id":"RANwtArN7HK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pd.DataFrame(final_logits)\n","result.index += 1\n","result.to_csv('/content/drive/MyDrive/to_submit/Results/Phase1/kcbert_00.csv', index_label=\"idx\")"],"metadata":{"id":"3hlw2nOJQ1wr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test = pd.DataFrame({\n","#     'I/E': [MBTI[0] for MBTI in MBTIs], \n","#     'S/N': [MBTI[1] for MBTI in MBTIs], \n","#     'T/F': [MBTI[2] for MBTI in MBTIs], \n","#     'J/P': [MBTI[3] for MBTI in MBTIs], \n","# })\n","# test.index += 1\n","# test.to_csv(f'/content/drive/MyDrive/QIA2023_phase1/result/single-flow-1/result-{checkpoint_idx}.csv', index_label=\"idx\")"],"metadata":{"id":"la7eFmwBYPeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %cp -av /content/checkpoints/checkpoint-500 /content/drive/MyDrive/QIA2023_phase1/checkpoint"],"metadata":{"id":"KhhErpBk-f4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OnP5nZqTBPvR"},"execution_count":null,"outputs":[]}]}